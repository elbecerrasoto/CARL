{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"dqn_forest_fire_v3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DEEJRU2QqY40"},"source":["# N-Step Forest Fire DQN"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Fd4qKf0yq5tG"},"source":["## Set Up"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"687DQnlzp8Vp"},"source":["### Colab Set Up"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"i5XIYlb7uqE-","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1592802470231,"user_tz":360,"elapsed":22093,"user":{"displayName":"Emanuel Becerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCu6ntkv7NB7qaOUqYqULgfRdXUsXmMbN0tWm--Q=s64","userId":"11342821288647339595"}},"outputId":"e2400e9b-c8ef-4d81-a8df-fb026ec19f3b"},"source":["# ------------- Colab Set Up ------------\n","\n","WORKING_DIRECTORY = '/gdrive/My Drive/CARL/RUNS2/06_21_9x9'\n","\n","# Mounting Drive at root\n","from google.colab import drive\n","drive.mount('/gdrive')\n","import os\n","os.chdir(WORKING_DIRECTORY)\n","# ! pip install seaborn"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"adE7f9mOqFnz"},"source":["### Libraries"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7kgtLKagv5Vv","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1592802476447,"user_tz":360,"elapsed":28292,"user":{"displayName":"Emanuel Becerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCu6ntkv7NB7qaOUqYqULgfRdXUsXmMbN0tWm--Q=s64","userId":"11342821288647339595"}},"outputId":"38d4ab0e-b8d1-47c6-a811-5aeb9bed98cb"},"source":["# ------------- Libraries ------------\n","\n","import os\n","import shutil\n","import pickle\n","import datetime\n","\n","from helicopter import EnvMakerForestFire\n","from lib import dqn_model\n","from lib.helpers import Agent, ReplayMemoryNSteps, get_epsilon, observations_to_tensors\n","\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","SEED = 125750\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f3c5ac41a90>"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ywLhTHVbqwrV"},"source":["## Algorithm Parameters"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cCSbPo57qQRZ"},"source":["### Running Params"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zzhAvJn02Jic","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1592802476451,"user_tz":360,"elapsed":28282,"user":{"displayName":"Emanuel Becerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCu6ntkv7NB7qaOUqYqULgfRdXUsXmMbN0tWm--Q=s64","userId":"11342821288647339595"}},"outputId":"9dcb9780-4258-4aaa-e054-4ca68fa6381e"},"source":["# ------------- Running Params -------------\n","\n","# Iterations of this particular run\n","JOB_ITERS = int(1.6e6)\n","# {'cpu', 'cuda'}, choose 'cuda' if hardware acceleration present\n","# DEVICE = torch.device('cpu')\n","DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using {DEVICE}')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4v80gQM1rGl6"},"source":["### Logging Params"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Eqe8So4O2T1y","colab":{},"executionInfo":{"status":"ok","timestamp":1592802476455,"user_tz":360,"elapsed":28282,"user":{"displayName":"Emanuel Becerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCu6ntkv7NB7qaOUqYqULgfRdXUsXmMbN0tWm--Q=s64","userId":"11342821288647339595"}}},"source":["# ------------- Logging Params ------------\n","# Printing mean reward each x epochs\n","EVAL_EPOCHS = int(1e4)\n","EVAL_ITERS = 1000\n","EVAL_FILE = 'dqn_eval.csv'\n","\n","# Iterations to log tmp file in case of a failure\n","LOG_TMP = int(1e4)\n","# Log to csv file each Iters\n","LOG_CSV = 10\n","# Logging Files\n","STATE_FILE = 'dqn_state.txt'\n","WEIGHTS_FILE = 'dqn_model_weights.pytorch'\n","TMP_WEIGHTS_OLD = 'dqn_model_weights_tmp_old.pytorch'\n","TMP_WEIGHTS_NEW = 'dqn_model_weights_tmp_new.pytorch'\n","TRAINING_FILE = 'dqn_training.csv'\n","MEMORY_FILE = 'dqn_buffer.pickle'\n","TMP_MEMORY_OLD = 'dqn_buffer_tmp_old.pickle'\n","TMP_MEMORY_NEW = 'dqn_buffer_tmp_new.pickle'\n","ENV_FILE = 'dqn_env.pickle'"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"f1JnBprCqm7X"},"source":["### Env Params"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dfogQe052Wwh","colab":{},"executionInfo":{"status":"ok","timestamp":1592802476458,"user_tz":360,"elapsed":28281,"user":{"displayName":"Emanuel Becerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCu6ntkv7NB7qaOUqYqULgfRdXUsXmMbN0tWm--Q=s64","userId":"11342821288647339595"}}},"source":["# ------------- Env Params ------------\n","RESET_ENV = EVAL_EPOCHS\n","\n","D_ENV = {}\n","for env_type in ('train','eval'):\n","  env = EnvMakerForestFire(observation_mode='channels3',\n","                         n_row=9, n_col=9,\n","                         p_fire=0.009, p_tree=0.300,\n","                         moves_before_updating=5,\n","                         reward_fire=-1.00, reward_tree=0.60, reward_empty=-0.30, reward_hit=2.00,\n","                         reward_type='both')\n","  D_ENV[env_type] = env\n","\n","ENV = D_ENV['train']\n","ENV_EVAL = D_ENV['eval']\n","\n","# Number of actions\n","N_ACTIONS = len(ENV.movement_actions)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9UcMjmE5qrW6"},"source":["### DQN Params"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Agi13dka2XEK","colab":{},"executionInfo":{"status":"ok","timestamp":1592802476461,"user_tz":360,"elapsed":28281,"user":{"displayName":"Emanuel Becerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCu6ntkv7NB7qaOUqYqULgfRdXUsXmMbN0tWm--Q=s64","userId":"11342821288647339595"}}},"source":["# ------------- DQN Params ------------\n","\n","# N-steps unrolling of the Bellman Equation\n","STEPS = 3\n","# Discount\n","GAMMA = 0.99\n","# Min possible epsilon on epsilon-greedy policy, exponetial decay\n","MIN_EPSILON = 0.10\n","# Exploration EPOCHS\n","EXPLORATION_EPOCHS = int(1e6)\n","USE_HEURISTIC = False\n","# Synchronize target network each x steps\n","SYNC_TARGET = int(1e4)\n","# Replay Memory Size\n","REPLAY_SIZE = int(0.8e6)\n","# Start learning at this size of the replay memory\n","REPLAY_START_SIZE = 2000"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9IY7Gi5Ur7qK"},"source":["### Network Params"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8czSeb9s2XUw","colab":{"base_uri":"https://localhost:8080/","height":287},"executionInfo":{"status":"ok","timestamp":1592802485973,"user_tz":360,"elapsed":37780,"user":{"displayName":"Emanuel Becerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCu6ntkv7NB7qaOUqYqULgfRdXUsXmMbN0tWm--Q=s64","userId":"11342821288647339595"}},"outputId":"2ddd7165-6e64-414d-cce1-85802e6a8faa"},"source":["# ------------- Network Params ------------\n","\n","# Training\n","BATCH_SIZE = 16\n","LEARNING_RATE = 2e-4\n","# Instantiating the network\n","def set_global_shape(env):\n","    grid, pos, moves = env.reset()\n","    return grid.shape\n","SHAPE = set_global_shape(ENV)\n","NET = dqn_model.DQN(SHAPE, N_ACTIONS).to(DEVICE)\n","print(NET)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["DQN(\n","  (conv): Sequential(\n","    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): GELU()\n","    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): GELU()\n","    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (5): GELU()\n","  )\n","  (fc): Sequential(\n","    (0): Linear(in_features=2595, out_features=256, bias=True)\n","    (1): GELU()\n","    (2): Linear(in_features=256, out_features=9, bias=True)\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Qbx0WE42sAJo"},"source":["## Resuming Execution and Loss"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IdgQXCuysPoy"},"source":["### Resuming Execution"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2r6IdbnY2Xjy","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1592802487401,"user_tz":360,"elapsed":39204,"user":{"displayName":"Emanuel Becerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCu6ntkv7NB7qaOUqYqULgfRdXUsXmMbN0tWm--Q=s64","userId":"11342821288647339595"}},"outputId":"9ed5223f-705d-4283-9bd8-52f9b2d2cb03"},"source":["# ------------- Resuming Execution ------------\n","\n","params_to_log=\\\n","f\"\"\"# FOREST FIRE DQN\n","# {datetime.datetime.now().strftime('%Y/%B/%d %H:%M:%S')}\n","#\n","# ------------- Running Params -------------\n","# JOB_ITERS = {JOB_ITERS}\n","# DEVICE = {str(DEVICE)}\n","#\n","# ------------- Logging Params ------------\n","# EVAL_EPOCHS = {EVAL_EPOCHS}\n","# EVAL_ITERS = {EVAL_ITERS}\n","# STATE_FILE = {STATE_FILE}\n","# TRAINING_FILE = {TRAINING_FILE}\n","# WEIGHTS_FILE = {WEIGHTS_FILE}\n","# EVAL_FILE = {EVAL_FILE}\n","#\n","# ------------- DQN Params ------------\n","# * N-steps unrolling of the Bellman Equation\n","# STEPS = {STEPS}\n","# * Discount\n","# GAMMA = {GAMMA}\n","# * Min possible epsilon on epsilon-greedy policy, exponetial decay\n","# MIN_EPSILON = {MIN_EPSILON}\n","# EXPLORATION_EPOCHS = {EXPLORATION_EPOCHS}\n","# USE_HEURISTIC = {USE_HEURISTIC}\n","# * Synchronize target network each x steps\n","# SYNC_TARGET = {SYNC_TARGET}\n","# * Replay Memory Size\n","# REPLAY_SIZE = {REPLAY_SIZE}\n","# * Start learning at this size of the replay memory\n","# REPLAY_START_SIZE = {REPLAY_START_SIZE}\n","#\n","# ------------- Network Params ------------\n","# BATCH_SIZE = {BATCH_SIZE}\n","# LEARNING_RATE = {LEARNING_RATE}\n","#\n","\"\"\"\n","\n","if os.path.isfile(STATE_FILE):\n","    # Loading the pretrained net\n","    if os.path.isfile(WEIGHTS_FILE):\n","        NET.load_state_dict(torch.load(WEIGHTS_FILE, map_location=DEVICE))\n","    net = NET\n","\n","    # Training File\n","    if not os.path.isfile(TRAINING_FILE):\n","      with open(TRAINING_FILE, 'a') as file:\n","          file.write('epoch,reward,loss,epsilon\\n')\n","\n","    # Loading memory replay\n","    if os.path.isfile(MEMORY_FILE):\n","        with open(MEMORY_FILE, 'rb') as file:\n","            buffer = pickle.load(file)\n","        agent = Agent(ENV, buffer)\n","    else:\n","        # Initializing Memory and Agent        \n","        buffer = ReplayMemoryNSteps(REPLAY_SIZE, steps=STEPS)\n","        agent = Agent(ENV, buffer)\n","        # Fill the replay memory\n","        for filling_step in range(REPLAY_START_SIZE):\n","            agent.play_step(net, epsilon = 1.00, device=DEVICE)\n","        print(f'\\nFilled Replay Memory')\n","\n","    with open(STATE_FILE, 'r') as file:\n","        lines = list(file)\n","        last_line = lines[-1]\n","        RESUMED_EPOCH = int(last_line.split(',')[0])+1\n","    with open(STATE_FILE, 'a') as file:\n","        date = datetime.datetime.now().strftime('%Y,%m,%d,%H,%M,%S')\n","        file.write(''.join((f'{RESUMED_EPOCH},START,', date, '\\n')))\n","else:\n","    # Initializing State File\n","    def comment_string(string, comment='# ', separator='\\n'):\n","        commented = []\n","        for line in string.split(separator):\n","            commented.append(comment + line)\n","        return commented\n","    env_params = repr(ENV.init_kw_params)\n","    net_arch = repr(NET)\n","    with open(STATE_FILE, 'w') as file:\n","        file.write(params_to_log)\n","        file.write('# ------------- Environment Params ------------\\n')\n","        commented = comment_string(env_params, separator=',')\n","        for idx, line in enumerate(commented):\n","            line = line + ',\\n' if idx != len(commented)-1 else line + '\\n'\n","            file.write(line)\n","        file.write('#\\n# ------------- Network Arch ------------\\n')\n","        for line in comment_string(net_arch):\n","            line += '\\n'\n","            file.write(line)\n","        file.write('epoch,bound,year,month,day,hour,minute,second\\n')\n","        date = datetime.datetime.now().strftime('%Y,%m,%d,%H,%M,%S')\n","        file.write(''.join(('0,START,', date, '\\n')))\n","\n","    # Training File\n","    with open(TRAINING_FILE, 'a') as file:\n","        file.write('epoch,reward,loss,epsilon\\n')\n","\n","    # Load Weights if available\n","    if os.path.isfile(WEIGHTS_FILE):\n","        NET.load_state_dict(torch.load(WEIGHTS_FILE, map_location=DEVICE))\n","    net = NET\n","    \n","    # Initializing Memory and Agent        \n","    buffer = ReplayMemoryNSteps(REPLAY_SIZE, steps=STEPS)\n","    agent = Agent(ENV, buffer)\n","\n","    # Fill the replay memory\n","    for filling_step in range(REPLAY_START_SIZE):\n","        agent.play_step(net, epsilon = 1.00, device=DEVICE)\n","    print(f'\\nFilled Replay Memory')\n","    \n","    RESUMED_EPOCH = 0\n","\n","if not os.path.isfile(EVAL_FILE):\n","    with open(EVAL_FILE, 'a') as file_eval:\n","      file_eval.write('epoch,mean_eval,mean_train,loss,epsilon\\n')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["\n","Filled Replay Memory\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rxXEQOhTsU1h"},"source":["### Loss"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BOqPkd522Xvf","colab":{},"executionInfo":{"status":"ok","timestamp":1592802487406,"user_tz":360,"elapsed":39205,"user":{"displayName":"Emanuel Becerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCu6ntkv7NB7qaOUqYqULgfRdXUsXmMbN0tWm--Q=s64","userId":"11342821288647339595"}}},"source":["# ------------- Helper Functions ------------\n","\n","def calc_loss(batch, net, tgt_net, gamma, device='cpu'):\n","    states, actions, rewards, dones, next_states = batch\n","    \n","    grids, positions, moves = observations_to_tensors(states, device=device)\n","    grids_next, positions_next, moves_next = observations_to_tensors(next_states, device=device)\n","    \n","    actions_v = torch.tensor(actions).to(device) - 1\n","    done_mask = torch.BoolTensor(dones).to(device)\n","\n","    # Quality of the taken actions\n","    state_action_values = net(grids, positions, moves).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n","    \n","    # Next state max Q(s,a)\n","    next_state_values = tgt_net(grids_next, positions_next, moves_next).max(1)[0]\n","    # The value of ended episode is 0\n","    next_state_values[done_mask] = 0.0\n","    # Detach in order to not to propagate gradients into tgt network\n","    next_state_values = next_state_values.detach()\n","    \n","    # Calculate target\n","    targets_batch = calc_batch_targets(rewards, gamma, next_state_values, device)\n","    return nn.MSELoss()(state_action_values, targets_batch)\n","\n","# N Steps Bellman Equation\n","def calc_batch_targets(rewards_batch, gamma, next_state_values, device='cpu'):\n","    targets_batch = []\n","    for idx_batch, rewards_steps in enumerate(rewards_batch):\n","        \n","        expected_q = 0.0\n","        for step, reward in enumerate(rewards_steps):\n","            expected_q += gamma**step * reward\n","        expected_q += gamma**(step+1) * next_state_values[idx_batch]\n","        targets_batch.append(expected_q)\n","\n","    targets_batch = torch.FloatTensor(targets_batch).to(device)\n","    return targets_batch"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Mg8rS3WvtttG"},"source":["## Main program"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"w0Ah-7FJscnv"},"source":["### Train Loop"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4eDFvMzU2X6y","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592844311553,"user_tz":360,"elapsed":18138790,"user":{"displayName":"Emanuel Becerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCu6ntkv7NB7qaOUqYqULgfRdXUsXmMbN0tWm--Q=s64","userId":"11342821288647339595"}},"outputId":"f6aee472-d742-4293-af33-b4789934cae8"},"source":["# ------------- Main Loop -------------\n","# Agent for Evaluation\n","AGENT_EVAL = Agent(ENV_EVAL, ReplayMemoryNSteps(42, steps=STEPS))\n","\n","# Initializations\n","tgt_net = dqn_model.DQN(SHAPE, N_ACTIONS).to(DEVICE)\n","tgt_net.load_state_dict(net.state_dict())\n","\n","optimizer = optim.Adam(net.parameters(), lr = LEARNING_RATE)\n","\n","rewards_x_steps = []\n","epoch = RESUMED_EPOCH\n","print('STARTING TO LEARN NOW', end='\\n\\n')\n","with open(TRAINING_FILE, 'a') as file:\n","    for i in range(JOB_ITERS):\n","        # Play a step\n","        epsilon = get_epsilon(epoch, exploration_epochs=EXPLORATION_EPOCHS, min_epsilon=MIN_EPSILON)\n","        if USE_HEURISTIC:\n","          if epoch < EXPLORATION_EPOCHS :\n","              reward = agent.play_step(net, epsilon, device=DEVICE, policy='heuristic')\n","          else:\n","              reward = agent.play_step(net, epsilon, device=DEVICE)\n","        else:\n","          reward = agent.play_step(net, epsilon, device=DEVICE)\n","        rewards_x_steps.append(reward)\n","        \n","        # Network Optimization\n","        optimizer.zero_grad()\n","        batch = buffer.sample(BATCH_SIZE)\n","        loss_t = calc_loss(batch, net, tgt_net, gamma=GAMMA, device=DEVICE)\n","        loss_t.backward()\n","        optimizer.step()\n","        \n","        # Log to CSV\n","        if epoch % LOG_CSV == 0:\n","            log_loss = np.round(loss_t.item(), 4)\n","            log_epsilon = np.round(epsilon, 4)\n","            log = str(epoch), str(reward), str(log_loss), str(log_epsilon)\n","            file.write(','.join(log) + '\\n')\n","\n","        if epoch % RESET_ENV == 0:\n","            agent._reset()\n","        \n","        epoch += 1\n","\n","        if epoch % EVAL_EPOCHS == 0:\n","          eval_rewards = []\n","          AGENT_EVAL._reset()\n","          for i in range(EVAL_ITERS):\n","            eval_rewards.append(AGENT_EVAL.play_step(net, device=DEVICE))\n","          eval_rewards_v = np.array(eval_rewards)\n","          eval_mean_reward = eval_rewards_v.mean()\n","          print(f'EPOCH: {epoch}')\n","          print('Mean reward per step (Evaluation): ' + str(np.round(eval_mean_reward,4)))\n","\n","          rewards_x_steps_v = np.array(rewards_x_steps)\n","          mean_reward = rewards_x_steps_v.mean()\n","          print('Mean reward per step (Training): ' + str(np.round(mean_reward,4)) + '\\n')\n","          rewards_x_steps = []\n","\n","          with open(EVAL_FILE,'a') as eval_file:\n","            log_loss = np.round(loss_t.item(), 4)\n","            log_epsilon = np.round(epsilon, 4)\n","            eval_log = str(epoch), str(np.round(eval_mean_reward,4)), str(np.round(mean_reward,4)), str(log_loss), str(log_epsilon)\n","            eval_file.write(','.join(eval_log) + '\\n')\n","\n","        # Syncronize net and target net\n","        if epoch % SYNC_TARGET == 0:\n","            tgt_net.load_state_dict(net.state_dict())\n","            \n","        # Logging temporal file in case of a failure\n","        if epoch % LOG_TMP == 0:\n","          if not os.path.isfile(TMP_WEIGHTS_OLD):\n","            # Temporal Weights\n","            torch.save(net.state_dict(), TMP_WEIGHTS_OLD)\n","            shutil.copyfile(TMP_WEIGHTS_OLD, TMP_WEIGHTS_NEW) \n","          else:\n","            os.remove(TMP_WEIGHTS_OLD)\n","            os.rename(TMP_WEIGHTS_NEW, TMP_WEIGHTS_OLD)\n","            torch.save(net.state_dict(), TMP_WEIGHTS_NEW)\n","          if not os.path.isfile(TMP_MEMORY_OLD):\n","            # Log memory buffer\n","            with open(TMP_MEMORY_OLD, 'wb') as file2:\n","                pickle.dump(agent.exp_buffer, file2)\n","            shutil.copyfile(TMP_MEMORY_OLD, TMP_MEMORY_NEW) \n","          else:\n","            os.remove(TMP_MEMORY_OLD)\n","            os.rename(TMP_MEMORY_NEW, TMP_MEMORY_OLD)\n","            with open(TMP_MEMORY_NEW, 'wb') as file2:\n","                pickle.dump(agent.exp_buffer, file2)\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["STARTING TO LEARN NOW\n","\n","EPOCH: 10000\n","Mean reward per step (Evaluation): -9.5315\n","Mean reward per step (Training): -9.7181\n","\n","EPOCH: 20000\n","Mean reward per step (Evaluation): -8.7795\n","Mean reward per step (Training): -9.7425\n","\n","EPOCH: 30000\n","Mean reward per step (Evaluation): -9.4073\n","Mean reward per step (Training): -9.7139\n","\n","EPOCH: 40000\n","Mean reward per step (Evaluation): -8.4529\n","Mean reward per step (Training): -10.1263\n","\n","EPOCH: 50000\n","Mean reward per step (Evaluation): -7.8312\n","Mean reward per step (Training): -9.7725\n","\n","EPOCH: 60000\n","Mean reward per step (Evaluation): -7.6179\n","Mean reward per step (Training): -9.9647\n","\n","EPOCH: 70000\n","Mean reward per step (Evaluation): -8.9433\n","Mean reward per step (Training): -9.8793\n","\n","EPOCH: 80000\n","Mean reward per step (Evaluation): -10.8728\n","Mean reward per step (Training): -9.6016\n","\n","EPOCH: 90000\n","Mean reward per step (Evaluation): -7.615\n","Mean reward per step (Training): -9.4254\n","\n","EPOCH: 100000\n","Mean reward per step (Evaluation): -4.905\n","Mean reward per step (Training): -10.0316\n","\n","EPOCH: 110000\n","Mean reward per step (Evaluation): -8.8082\n","Mean reward per step (Training): -10.0747\n","\n","EPOCH: 120000\n","Mean reward per step (Evaluation): -8.9243\n","Mean reward per step (Training): -10.1583\n","\n","EPOCH: 130000\n","Mean reward per step (Evaluation): -9.187\n","Mean reward per step (Training): -9.7159\n","\n","EPOCH: 140000\n","Mean reward per step (Evaluation): -9.4644\n","Mean reward per step (Training): -10.297\n","\n","EPOCH: 150000\n","Mean reward per step (Evaluation): -10.5625\n","Mean reward per step (Training): -9.9394\n","\n","EPOCH: 160000\n","Mean reward per step (Evaluation): -6.3537\n","Mean reward per step (Training): -9.9575\n","\n","EPOCH: 170000\n","Mean reward per step (Evaluation): -11.2689\n","Mean reward per step (Training): -9.9358\n","\n","EPOCH: 180000\n","Mean reward per step (Evaluation): -8.7854\n","Mean reward per step (Training): -9.9988\n","\n","EPOCH: 190000\n","Mean reward per step (Evaluation): -7.4292\n","Mean reward per step (Training): -10.0281\n","\n","EPOCH: 200000\n","Mean reward per step (Evaluation): -9.4928\n","Mean reward per step (Training): -10.2188\n","\n","EPOCH: 210000\n","Mean reward per step (Evaluation): -8.8661\n","Mean reward per step (Training): -9.8502\n","\n","EPOCH: 220000\n","Mean reward per step (Evaluation): -8.7739\n","Mean reward per step (Training): -9.6673\n","\n","EPOCH: 230000\n","Mean reward per step (Evaluation): -9.9619\n","Mean reward per step (Training): -10.0159\n","\n","EPOCH: 240000\n","Mean reward per step (Evaluation): -10.2043\n","Mean reward per step (Training): -10.2914\n","\n","EPOCH: 250000\n","Mean reward per step (Evaluation): -7.9371\n","Mean reward per step (Training): -9.6154\n","\n","EPOCH: 260000\n","Mean reward per step (Evaluation): -9.6633\n","Mean reward per step (Training): -10.427\n","\n","EPOCH: 270000\n","Mean reward per step (Evaluation): -10.3291\n","Mean reward per step (Training): -9.8202\n","\n","EPOCH: 280000\n","Mean reward per step (Evaluation): -8.6002\n","Mean reward per step (Training): -10.4465\n","\n","EPOCH: 290000\n","Mean reward per step (Evaluation): -8.3231\n","Mean reward per step (Training): -10.08\n","\n","EPOCH: 300000\n","Mean reward per step (Evaluation): -9.4642\n","Mean reward per step (Training): -10.0187\n","\n","EPOCH: 310000\n","Mean reward per step (Evaluation): -9.8317\n","Mean reward per step (Training): -9.9135\n","\n","EPOCH: 320000\n","Mean reward per step (Evaluation): -10.1549\n","Mean reward per step (Training): -10.127\n","\n","EPOCH: 330000\n","Mean reward per step (Evaluation): -10.9389\n","Mean reward per step (Training): -10.1694\n","\n","EPOCH: 340000\n","Mean reward per step (Evaluation): -6.0916\n","Mean reward per step (Training): -10.6581\n","\n","EPOCH: 350000\n","Mean reward per step (Evaluation): -10.6796\n","Mean reward per step (Training): -10.391\n","\n","EPOCH: 360000\n","Mean reward per step (Evaluation): -8.2803\n","Mean reward per step (Training): -10.3865\n","\n","EPOCH: 370000\n","Mean reward per step (Evaluation): -8.3786\n","Mean reward per step (Training): -9.6528\n","\n","EPOCH: 380000\n","Mean reward per step (Evaluation): -9.5038\n","Mean reward per step (Training): -10.0726\n","\n","EPOCH: 390000\n","Mean reward per step (Evaluation): -9.9314\n","Mean reward per step (Training): -10.2217\n","\n","EPOCH: 400000\n","Mean reward per step (Evaluation): -8.659\n","Mean reward per step (Training): -10.138\n","\n","EPOCH: 410000\n","Mean reward per step (Evaluation): -6.0049\n","Mean reward per step (Training): -10.0563\n","\n","EPOCH: 420000\n","Mean reward per step (Evaluation): -9.8814\n","Mean reward per step (Training): -9.6526\n","\n","EPOCH: 430000\n","Mean reward per step (Evaluation): -10.1669\n","Mean reward per step (Training): -10.1114\n","\n","EPOCH: 440000\n","Mean reward per step (Evaluation): -9.201\n","Mean reward per step (Training): -9.8859\n","\n","EPOCH: 450000\n","Mean reward per step (Evaluation): -8.3613\n","Mean reward per step (Training): -9.7884\n","\n","EPOCH: 460000\n","Mean reward per step (Evaluation): -8.4382\n","Mean reward per step (Training): -10.2704\n","\n","EPOCH: 470000\n","Mean reward per step (Evaluation): -6.8066\n","Mean reward per step (Training): -10.0069\n","\n","EPOCH: 480000\n","Mean reward per step (Evaluation): -6.8821\n","Mean reward per step (Training): -9.6342\n","\n","EPOCH: 490000\n","Mean reward per step (Evaluation): -9.9403\n","Mean reward per step (Training): -10.1946\n","\n","EPOCH: 500000\n","Mean reward per step (Evaluation): -9.5477\n","Mean reward per step (Training): -10.3247\n","\n","EPOCH: 510000\n","Mean reward per step (Evaluation): -9.3513\n","Mean reward per step (Training): -10.1996\n","\n","EPOCH: 520000\n","Mean reward per step (Evaluation): -8.4152\n","Mean reward per step (Training): -9.8565\n","\n","EPOCH: 530000\n","Mean reward per step (Evaluation): -10.055\n","Mean reward per step (Training): -9.847\n","\n","EPOCH: 540000\n","Mean reward per step (Evaluation): -9.5148\n","Mean reward per step (Training): -10.1541\n","\n","EPOCH: 550000\n","Mean reward per step (Evaluation): -7.5768\n","Mean reward per step (Training): -10.0639\n","\n","EPOCH: 560000\n","Mean reward per step (Evaluation): -8.1388\n","Mean reward per step (Training): -10.4904\n","\n","EPOCH: 570000\n","Mean reward per step (Evaluation): -8.7944\n","Mean reward per step (Training): -9.6491\n","\n","EPOCH: 580000\n","Mean reward per step (Evaluation): -8.4759\n","Mean reward per step (Training): -10.2002\n","\n","EPOCH: 590000\n","Mean reward per step (Evaluation): -7.5356\n","Mean reward per step (Training): -9.5244\n","\n","EPOCH: 600000\n","Mean reward per step (Evaluation): -9.7154\n","Mean reward per step (Training): -10.158\n","\n","EPOCH: 610000\n","Mean reward per step (Evaluation): -7.3815\n","Mean reward per step (Training): -10.2361\n","\n","EPOCH: 620000\n","Mean reward per step (Evaluation): -8.7528\n","Mean reward per step (Training): -10.2242\n","\n","EPOCH: 630000\n","Mean reward per step (Evaluation): -10.1595\n","Mean reward per step (Training): -10.2632\n","\n","EPOCH: 640000\n","Mean reward per step (Evaluation): -8.506\n","Mean reward per step (Training): -10.0966\n","\n","EPOCH: 650000\n","Mean reward per step (Evaluation): -9.8393\n","Mean reward per step (Training): -10.0266\n","\n","EPOCH: 660000\n","Mean reward per step (Evaluation): -9.23\n","Mean reward per step (Training): -9.732\n","\n","EPOCH: 670000\n","Mean reward per step (Evaluation): -8.7187\n","Mean reward per step (Training): -9.7255\n","\n","EPOCH: 680000\n","Mean reward per step (Evaluation): -7.1867\n","Mean reward per step (Training): -10.5219\n","\n","EPOCH: 690000\n","Mean reward per step (Evaluation): -8.6337\n","Mean reward per step (Training): -10.0938\n","\n","EPOCH: 700000\n","Mean reward per step (Evaluation): -8.9637\n","Mean reward per step (Training): -10.3975\n","\n","EPOCH: 710000\n","Mean reward per step (Evaluation): -7.0922\n","Mean reward per step (Training): -10.1235\n","\n","EPOCH: 720000\n","Mean reward per step (Evaluation): -9.3216\n","Mean reward per step (Training): -10.1337\n","\n","EPOCH: 730000\n","Mean reward per step (Evaluation): -10.0092\n","Mean reward per step (Training): -10.2343\n","\n","EPOCH: 740000\n","Mean reward per step (Evaluation): -10.0131\n","Mean reward per step (Training): -10.3358\n","\n","EPOCH: 750000\n","Mean reward per step (Evaluation): -6.2763\n","Mean reward per step (Training): -9.8265\n","\n","EPOCH: 760000\n","Mean reward per step (Evaluation): -7.6885\n","Mean reward per step (Training): -9.5727\n","\n","EPOCH: 770000\n","Mean reward per step (Evaluation): -9.3744\n","Mean reward per step (Training): -10.5349\n","\n","EPOCH: 780000\n","Mean reward per step (Evaluation): -8.3384\n","Mean reward per step (Training): -10.041\n","\n","EPOCH: 790000\n","Mean reward per step (Evaluation): -9.4921\n","Mean reward per step (Training): -9.919\n","\n","EPOCH: 800000\n","Mean reward per step (Evaluation): -7.8024\n","Mean reward per step (Training): -10.4985\n","\n","EPOCH: 810000\n","Mean reward per step (Evaluation): -7.9671\n","Mean reward per step (Training): -10.0754\n","\n","EPOCH: 820000\n","Mean reward per step (Evaluation): -7.2987\n","Mean reward per step (Training): -10.1901\n","\n","EPOCH: 830000\n","Mean reward per step (Evaluation): -5.5209\n","Mean reward per step (Training): -10.017\n","\n","EPOCH: 840000\n","Mean reward per step (Evaluation): -11.2859\n","Mean reward per step (Training): -10.2163\n","\n","EPOCH: 850000\n","Mean reward per step (Evaluation): -9.2529\n","Mean reward per step (Training): -10.4321\n","\n","EPOCH: 860000\n","Mean reward per step (Evaluation): -9.448\n","Mean reward per step (Training): -9.8623\n","\n","EPOCH: 870000\n","Mean reward per step (Evaluation): -6.6343\n","Mean reward per step (Training): -9.7971\n","\n","EPOCH: 880000\n","Mean reward per step (Evaluation): -10.0499\n","Mean reward per step (Training): -10.7918\n","\n","EPOCH: 890000\n","Mean reward per step (Evaluation): -8.3718\n","Mean reward per step (Training): -10.3943\n","\n","EPOCH: 900000\n","Mean reward per step (Evaluation): -9.4239\n","Mean reward per step (Training): -10.238\n","\n","EPOCH: 910000\n","Mean reward per step (Evaluation): -9.5127\n","Mean reward per step (Training): -10.1598\n","\n","EPOCH: 920000\n","Mean reward per step (Evaluation): -9.773\n","Mean reward per step (Training): -9.942\n","\n","EPOCH: 930000\n","Mean reward per step (Evaluation): -7.6453\n","Mean reward per step (Training): -9.7047\n","\n","EPOCH: 940000\n","Mean reward per step (Evaluation): -6.1526\n","Mean reward per step (Training): -9.8721\n","\n","EPOCH: 950000\n","Mean reward per step (Evaluation): -7.5102\n","Mean reward per step (Training): -10.1643\n","\n","EPOCH: 960000\n","Mean reward per step (Evaluation): -7.849\n","Mean reward per step (Training): -9.9833\n","\n","EPOCH: 970000\n","Mean reward per step (Evaluation): -9.2489\n","Mean reward per step (Training): -10.455\n","\n","EPOCH: 980000\n","Mean reward per step (Evaluation): -7.5173\n","Mean reward per step (Training): -10.5031\n","\n","EPOCH: 990000\n","Mean reward per step (Evaluation): -9.8846\n","Mean reward per step (Training): -10.4228\n","\n","EPOCH: 1000000\n","Mean reward per step (Evaluation): -9.5403\n","Mean reward per step (Training): -10.5955\n","\n","EPOCH: 1010000\n","Mean reward per step (Evaluation): -8.256\n","Mean reward per step (Training): -10.4501\n","\n","EPOCH: 1020000\n","Mean reward per step (Evaluation): -9.0569\n","Mean reward per step (Training): -10.2599\n","\n","EPOCH: 1030000\n","Mean reward per step (Evaluation): -9.4818\n","Mean reward per step (Training): -10.6464\n","\n","EPOCH: 1040000\n","Mean reward per step (Evaluation): -5.8823\n","Mean reward per step (Training): -9.876\n","\n","EPOCH: 1050000\n","Mean reward per step (Evaluation): -8.2554\n","Mean reward per step (Training): -10.3562\n","\n","EPOCH: 1060000\n","Mean reward per step (Evaluation): -9.0782\n","Mean reward per step (Training): -10.3839\n","\n","EPOCH: 1070000\n","Mean reward per step (Evaluation): -8.7891\n","Mean reward per step (Training): -9.8662\n","\n","EPOCH: 1080000\n","Mean reward per step (Evaluation): -10.5444\n","Mean reward per step (Training): -10.1172\n","\n","EPOCH: 1090000\n","Mean reward per step (Evaluation): -9.4\n","Mean reward per step (Training): -10.2717\n","\n","EPOCH: 1100000\n","Mean reward per step (Evaluation): -9.6014\n","Mean reward per step (Training): -9.8624\n","\n","EPOCH: 1110000\n","Mean reward per step (Evaluation): -9.6742\n","Mean reward per step (Training): -10.2146\n","\n","EPOCH: 1120000\n","Mean reward per step (Evaluation): -9.0829\n","Mean reward per step (Training): -9.7896\n","\n","EPOCH: 1130000\n","Mean reward per step (Evaluation): -9.7834\n","Mean reward per step (Training): -9.6336\n","\n","EPOCH: 1140000\n","Mean reward per step (Evaluation): -8.7454\n","Mean reward per step (Training): -10.5563\n","\n","EPOCH: 1150000\n","Mean reward per step (Evaluation): -8.3483\n","Mean reward per step (Training): -10.013\n","\n","EPOCH: 1160000\n","Mean reward per step (Evaluation): -9.1262\n","Mean reward per step (Training): -10.3657\n","\n","EPOCH: 1170000\n","Mean reward per step (Evaluation): -11.2349\n","Mean reward per step (Training): -10.198\n","\n","EPOCH: 1180000\n","Mean reward per step (Evaluation): -8.6107\n","Mean reward per step (Training): -9.859\n","\n","EPOCH: 1190000\n","Mean reward per step (Evaluation): -10.2869\n","Mean reward per step (Training): -10.6452\n","\n","EPOCH: 1200000\n","Mean reward per step (Evaluation): -9.6862\n","Mean reward per step (Training): -10.14\n","\n","EPOCH: 1210000\n","Mean reward per step (Evaluation): -6.2347\n","Mean reward per step (Training): -10.5024\n","\n","EPOCH: 1220000\n","Mean reward per step (Evaluation): -9.5527\n","Mean reward per step (Training): -10.6896\n","\n","EPOCH: 1230000\n","Mean reward per step (Evaluation): -7.7421\n","Mean reward per step (Training): -9.8033\n","\n","EPOCH: 1240000\n","Mean reward per step (Evaluation): -7.8454\n","Mean reward per step (Training): -10.4322\n","\n","EPOCH: 1250000\n","Mean reward per step (Evaluation): -8.077\n","Mean reward per step (Training): -10.061\n","\n","EPOCH: 1260000\n","Mean reward per step (Evaluation): -9.5249\n","Mean reward per step (Training): -10.2454\n","\n","EPOCH: 1270000\n","Mean reward per step (Evaluation): -8.5235\n","Mean reward per step (Training): -10.3634\n","\n","EPOCH: 1280000\n","Mean reward per step (Evaluation): -9.365\n","Mean reward per step (Training): -10.0356\n","\n","EPOCH: 1290000\n","Mean reward per step (Evaluation): -8.5571\n","Mean reward per step (Training): -10.4573\n","\n","EPOCH: 1300000\n","Mean reward per step (Evaluation): -8.2701\n","Mean reward per step (Training): -10.4746\n","\n","EPOCH: 1310000\n","Mean reward per step (Evaluation): -8.1652\n","Mean reward per step (Training): -10.162\n","\n","EPOCH: 1320000\n","Mean reward per step (Evaluation): -9.9866\n","Mean reward per step (Training): -10.1779\n","\n","EPOCH: 1330000\n","Mean reward per step (Evaluation): -9.9169\n","Mean reward per step (Training): -9.5204\n","\n","EPOCH: 1340000\n","Mean reward per step (Evaluation): -10.7256\n","Mean reward per step (Training): -10.2472\n","\n","EPOCH: 1350000\n","Mean reward per step (Evaluation): -8.7568\n","Mean reward per step (Training): -9.7766\n","\n","EPOCH: 1360000\n","Mean reward per step (Evaluation): -10.0547\n","Mean reward per step (Training): -10.5529\n","\n","EPOCH: 1370000\n","Mean reward per step (Evaluation): -9.5944\n","Mean reward per step (Training): -10.1238\n","\n","EPOCH: 1380000\n","Mean reward per step (Evaluation): -7.3786\n","Mean reward per step (Training): -10.547\n","\n","EPOCH: 1390000\n","Mean reward per step (Evaluation): -8.9096\n","Mean reward per step (Training): -9.7736\n","\n","EPOCH: 1400000\n","Mean reward per step (Evaluation): -9.374\n","Mean reward per step (Training): -10.0562\n","\n","EPOCH: 1410000\n","Mean reward per step (Evaluation): -7.5035\n","Mean reward per step (Training): -9.7234\n","\n","EPOCH: 1420000\n","Mean reward per step (Evaluation): -9.7721\n","Mean reward per step (Training): -10.1756\n","\n","EPOCH: 1430000\n","Mean reward per step (Evaluation): -8.319\n","Mean reward per step (Training): -10.1494\n","\n","EPOCH: 1440000\n","Mean reward per step (Evaluation): -7.5644\n","Mean reward per step (Training): -10.0289\n","\n","EPOCH: 1450000\n","Mean reward per step (Evaluation): -7.9604\n","Mean reward per step (Training): -10.4966\n","\n","EPOCH: 1460000\n","Mean reward per step (Evaluation): -9.9931\n","Mean reward per step (Training): -10.6191\n","\n","EPOCH: 1470000\n","Mean reward per step (Evaluation): -9.1397\n","Mean reward per step (Training): -9.9441\n","\n","EPOCH: 1480000\n","Mean reward per step (Evaluation): -10.4668\n","Mean reward per step (Training): -10.2709\n","\n","EPOCH: 1490000\n","Mean reward per step (Evaluation): -7.5855\n","Mean reward per step (Training): -10.9563\n","\n","EPOCH: 1500000\n","Mean reward per step (Evaluation): -7.6735\n","Mean reward per step (Training): -10.4557\n","\n","EPOCH: 1510000\n","Mean reward per step (Evaluation): -10.3392\n","Mean reward per step (Training): -10.0752\n","\n","EPOCH: 1520000\n","Mean reward per step (Evaluation): -6.9748\n","Mean reward per step (Training): -10.4037\n","\n","EPOCH: 1530000\n","Mean reward per step (Evaluation): -9.15\n","Mean reward per step (Training): -10.2581\n","\n","EPOCH: 1540000\n","Mean reward per step (Evaluation): -6.8579\n","Mean reward per step (Training): -10.1557\n","\n","EPOCH: 1550000\n","Mean reward per step (Evaluation): -8.3974\n","Mean reward per step (Training): -10.621\n","\n","EPOCH: 1560000\n","Mean reward per step (Evaluation): -9.0433\n","Mean reward per step (Training): -9.9034\n","\n","EPOCH: 1570000\n","Mean reward per step (Evaluation): -8.2078\n","Mean reward per step (Training): -10.348\n","\n","EPOCH: 1580000\n","Mean reward per step (Evaluation): -6.68\n","Mean reward per step (Training): -10.3511\n","\n","EPOCH: 1590000\n","Mean reward per step (Evaluation): -6.3513\n","Mean reward per step (Training): -10.0582\n","\n","EPOCH: 1600000\n","Mean reward per step (Evaluation): -8.5607\n","Mean reward per step (Training): -10.2363\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Fxsd7F3JsfW0"},"source":["### Log Final Results"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wqVq3NKKpgSx","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1592844359921,"user_tz":360,"elapsed":48381,"user":{"displayName":"Emanuel Becerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCu6ntkv7NB7qaOUqYqULgfRdXUsXmMbN0tWm--Q=s64","userId":"11342821288647339595"}},"outputId":"8e6d4559-8bf9-48a2-9fa1-1129604c3047"},"source":["# ------------- Final Loggings -------------\n","\n","# Saving the learned weights\n","date = datetime.datetime.now().strftime('%Y_%B_%d_%H-%M-%S')\n","# Historic Weights\n","torch.save(net.state_dict(), date + '_' + WEIGHTS_FILE)\n","print(f'Writing learned weights to {WEIGHTS_FILE}')\n","# Last Weights\n","torch.save(net.state_dict(), WEIGHTS_FILE)\n","\n","# Log final Job Epoch\n","with open(STATE_FILE, 'a') as file:\n","        date = datetime.datetime.now().strftime('%Y,%m,%d,%H,%M,%S')\n","        file.write(''.join((f'{epoch-1},END,', date, '\\n')))\n","\n","# Log memory buffer\n","with open(MEMORY_FILE, 'wb') as file:\n","    pickle.dump(agent.exp_buffer, file)\n","    \n","# Log Env\n","with open(ENV_FILE, 'wb') as file:\n","    pickle.dump(ENV, file)\n","\n","print(f'Finished {JOB_ITERS} iterations')\n","print('JOB TERMINATED')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Writing learned weights to dqn_model_weights.pytorch\n","Finished 1600000 iterations\n","JOB TERMINATED\n"],"name":"stdout"}]}]}